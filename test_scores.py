# -*- coding: utf-8 -*-
"""Test-Scores.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CPq2W64dPb2blPjEarVYpcoGxXexbI-v

# Mount Google Drive
"""

# from google.colab import drive
# drive.mount('/content/drive')

"""# Import Libraries"""

import pandas as pd
import numpy as np
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import mean_squared_error

import matplotlib.pyplot as plt

"""# Load Data"""

train_df = pd.read_csv("./train.csv")
test_df = pd.read_csv("./test.csv")

TARGET = "exam_score"

"""Feature Split"""

categorical_cols = [
    "gender",
    "course",
    "internet_access",
    "sleep_quality",
    "study_method",
    "facility_rating",
    "exam_difficulty"
]

numerical_cols = [
    "age",
    "study_hours",
    "class_attendance",
    "sleep_hours"
]

"""Encode Categorical Features"""

encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    train_df[col] = le.fit_transform(train_df[col])
    test_df[col] = le.transform(test_df[col])
    encoders[col] = le

"""Scale Numerical Features"""

scaler = StandardScaler()

train_df[numerical_cols] = scaler.fit_transform(train_df[numerical_cols])
test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])

"""Train Validation Split"""

X = train_df[categorical_cols + numerical_cols]
y = train_df[TARGET]

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""# Build Model"""

inputs = []
embeddings = []

for col in categorical_cols:
    input_layer = tf.keras.Input(shape=(1,))
    vocab_size = train_df[col].nunique()
    embed_dim = min(50, vocab_size // 2 + 1)

    embed = tf.keras.layers.Embedding(vocab_size, embed_dim)(input_layer)
    embed = tf.keras.layers.Flatten()(embed)

    inputs.append(input_layer)
    embeddings.append(embed)

num_input = tf.keras.Input(shape=(len(numerical_cols),))
inputs.append(num_input)
embeddings.append(num_input)

x = tf.keras.layers.Concatenate()(embeddings)

x = tf.keras.layers.Dense(256, activation="relu")(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dropout(0.3)(x)

x = tf.keras.layers.Dense(128, activation="relu")(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dropout(0.3)(x)

x = tf.keras.layers.Dense(64, activation="relu")(x)

output = tf.keras.layers.Dense(1, activation="linear")(x)

model = tf.keras.Model(inputs=inputs, outputs=output)

"""Run Model"""

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss="mse",
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

model.summary()

"""Callbacks"""

callbacks = [
    tf.keras.callbacks.EarlyStopping(
        patience=10,
        restore_best_weights=True
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        patience=5,
        factor=0.5
    )
]

"""# Train Model"""

history = model.fit(
    [X_train[col] for col in categorical_cols] + [X_train[numerical_cols]],
    y_train,
    validation_data=(
        [X_val[col] for col in categorical_cols] + [X_val[numerical_cols]],
        y_val
    ),
    epochs=200,
    batch_size=256,
    callbacks=callbacks,
    verbose=1
)

"""# Validation Loss"""

plt.figure(figsize=(8,5))
plt.plot(history.history["loss"], label="Training Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("MSE Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.show()

"""# Submission"""

test_inputs = [test_df[col] for col in categorical_cols] + [test_df[numerical_cols]]

predictions = model.predict(test_inputs).flatten()

submission = pd.DataFrame({
    "id": test_df["id"],
    "exam_score": predictions
})

submission.to_csv("submission.csv", index=False)

"""Save model"""

model.save("student_score_model.keras")

"""Save preprocessors"""

import joblib

joblib.dump(encoders, "label_encoders.pkl")
joblib.dump(scaler, "scaler.pkl")

def preprocess_input(df):
    for col, le in encoders.items():
        df[col] = le.transform(df[col])
    df[numerical_cols] = scaler.transform(df[numerical_cols])
    return df

